"""
RAG-—Å–µ—Ä–≤–∏—Å –¥–ª—è –∏–Ω–∂–µ–Ω–µ—Ä–Ω–æ–≥–æ —á–∞—Ç-–±–æ—Ç–∞.
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Hugging Face Router API (OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π) –¥–ª—è LLM –∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ embeddings.
–ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: Qwen/Qwen2.5-VL-72B-Instruct:ovhcloud
"""

import os
import time
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional

try:
    from langchain_chroma import Chroma
except ImportError:
    from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.messages import HumanMessage, AIMessage
from dotenv import load_dotenv
from langfuse import Langfuse

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv()

CHROMA_PERSIST_DIR = Path(__file__).parent.parent / "chroma_db"
COLLECTION_NAME = "engineer_bot"
EMBEDDING_MODEL = "cointegrated/rubert-tiny2"

# —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ Hugging Face –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π) :
# - Qwen/Qwen2.5-VL-72B-Instruct:ovhcloud (vision-–º–æ–¥–µ–ª—å, –æ—Ç–ª–∏—á–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ)
# - meta-llama/Llama-3.1-8B-Instruct (–±—ã—Å—Ç—Ä–µ–µ, —Å—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å)
# - mistralai/Mistral-7B-Instruct-v0.2 (–±—ã—Å—Ç—Ä–∞—è)

DEFAULT_LLM_MODEL = "Qwen/Qwen2.5-VL-72B-Instruct:ovhcloud"

# —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–±–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏, –æ–Ω–∞ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ MessagesPlaceholder)
SYSTEM_PROMPT = """
–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –ø—Ä–∏—ë–º–æ-—Å–¥–∞—Ç–æ—á–Ω—ã–º –∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–æ–Ω–Ω—ã–º –∏—Å–ø—ã—Ç–∞–Ω–∏—è–º —ç–ª–µ–∫—Ç—Ä–æ—É—Å—Ç–∞–Ω–æ–≤–æ–∫. 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ 
–∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ (–ü–£–≠, –ü–¢–≠–≠–ü, –°–ü, –ì–û–°–¢, –†–î) –∏ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤.

–ü—Ä–∞–≤–∏–ª–∞:
1. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–∏–º–µ—Ä–æ–≤ –æ—Ç—á—ë—Ç–æ–≤
2. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
3. –í—Å–µ–≥–¥–∞ —Ü–∏—Ç–∏—Ä—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–Ω–∞–∑–≤–∞–Ω–∏–µ, –Ω–æ–º–µ—Ä, –ø—É–Ω–∫—Ç/—Ä–∞–∑–¥–µ–ª) –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ
4. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, –∏—Å–ø–æ–ª—å–∑—É–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ
5. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç —Ä–∞—Å—á–µ—Ç–æ–≤, –ø—Ä–∏–≤–µ–¥–∏ —Ñ–æ—Ä–º—É–ª—ã –∏ –æ–±—ä—è—Å–Ω–∏ —à–∞–≥–∏
6. –£—á–∏—Ç—ã–≤–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –¥–∏–∞–ª–æ–≥–µ, –µ—Å–ª–∏ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ
7. –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –í–°–ï –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã - –Ω—É–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –ª—é–±–æ–º –∏–∑ –Ω–∏—Ö
8. –ü—Ä–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑–º–µ—Ä–µ–Ω–∏–π —Å—Ä–∞–≤–Ω–∏–≤–∞–π —Å –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –∏ –¥–µ–ª–∞–π –≤—ã–≤–æ–¥ –æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏/–Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏
9. –ü—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞—Ö - –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–º–µ—Ä—ã –∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤ –¥–ª—è –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–∏
10. –ü—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö - –∏—â–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö (–ü–£–≠, –ü–¢–≠–≠–ü, –°–ü, –ì–û–°–¢, –†–î)

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã:
- –ü—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–∏ –∑–∞–∑–µ–º–ª–µ–Ω–∏—è - –∏—â–∏ –≤ –ü–£–≠ 1.8.39, –†–î 34.45-51.300-97
- –ü—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–∏ –∏–∑–æ–ª—è—Ü–∏–∏ - –∏—â–∏ –≤ –ü–£–≠ –ø.1.8.40
- –ü—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ–± –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã–∫–ª—é—á–∞—Ç–µ–ª—è—Ö - –∏—â–∏ –≤ –ü–£–≠ –ø.1.8.37
- –ü—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ —Ü–µ–ø–∏ —Ñ–∞–∑–∞-–Ω—É–ª—å - –∏—â–∏ –≤ –ü–¢–≠–≠–ü –ü—Ä–∏–ª. 3 –ø.28.4
- –ü—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞—Ö –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è - –∏—Å–ø–æ–ª—å–∑—É–π —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ –∏ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –æ—Ç—á—ë—Ç–æ–≤
- –ü—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ - –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–º–µ—Ä—ã –∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
{context}
"""

class RAGService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è RAG-–ø–æ–∏—Å–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ Hugging Face Inference API"""
    
    def __init__(
        self,
        model_name: Optional[str] = None,
        temperature: float = 0.1,
        k: int = 5,  # —É–≤–µ–ª–∏—á–µ–Ω–æ —Å 3 –¥–æ 5 –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        enable_langfuse: bool = True,
        hf_token: Optional[str] = None,
        enable_memory: bool = True
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-—Å–µ—Ä–≤–∏—Å–∞.
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Hugging Face (–Ω–∞–ø—Ä–∏–º–µ—Ä, "mistralai/Mistral-7B-Instruct-v0.2")
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–Ω–∏–∂–µ = –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ)
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            enable_langfuse: –í–∫–ª—é—á–∏—Ç—å —Ç—Ä–µ–π—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Langfuse
            hf_token: Hugging Face API —Ç–æ–∫–µ–Ω (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ HUGGINGFACE_API_TOKEN)
            enable_memory: –í–∫–ª—é—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ (memory)
        """
        self.k = k if k >= 3 else 5  # –º–∏–Ω–∏–º—É–º 5 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è (–±—ã–ª–æ 3)
        self.model_name = model_name or DEFAULT_LLM_MODEL
        self.temperature = temperature
        self.enable_langfuse = enable_langfuse
        self.enable_memory = enable_memory
        # –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        self.hf_token = hf_token or os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_API_TOKEN")
        
        if not self.hf_token:
            raise ValueError(
                "HF_TOKEN –∏–ª–∏ HUGGINGFACE_API_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"
                "–ø–æ–ª—É—á–∏—Ç–µ —Ç–æ–∫–µ–Ω –Ω–∞ https://huggingface.co/settings/tokens"
            )
        
        self.memory = None
        if self.enable_memory:
            self.memory = ChatMessageHistory()
            logger.info("–∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ (memory) –≤–∫–ª—é—á–µ–Ω–∞")
        
        self.langfuse = None
        if self.enable_langfuse:
            self._init_langfuse()
        
        self._init_vectorstore()
        
        self._init_llm()

        self._init_chain()
    
    def _init_langfuse(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Langfuse –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞"""
        public_key = os.getenv("LANGFUSE_PUBLIC_KEY")
        secret_key = os.getenv("LANGFUSE_SECRET_KEY")
        host = os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")
        
        if public_key and secret_key:
            try:
                self.langfuse = Langfuse(
                    public_key=public_key,
                    secret_key=secret_key,
                    host=host
                )
                print("Langfuse –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Langfuse: {e}")
                print("   –¢—Ä–µ–π—Å–∏–Ω–≥ –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω")
                self.langfuse = None
                self.enable_langfuse = False
        else:
            print("Langfuse –∫–ª—é—á–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
            print("–¢—Ä–µ–π—Å–∏–Ω–≥ –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω. –ù—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å LANGFUSE_PUBLIC_KEY –∏ LANGFUSE_SECRET_KEY")
            self.enable_langfuse = False
    
    def _init_vectorstore(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö Chroma"""
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        self.vectorstore = Chroma(
            persist_directory=str(CHROMA_PERSIST_DIR),
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME
        )
        
        self.retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": self.k}
        )
    
    def _init_llm(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM —á–µ—Ä–µ–∑ HF Router API (OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π)"""
        try:
            # –∏—Å–ø–æ–ª—å–∑—É–µ–º OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API —á–µ—Ä–µ–∑ Hugging Face Router
            # –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ client = OpenAI(base_url="https://router.huggingface.co/v1", api_key=os.environ["HF_TOKEN"])
            self.llm = ChatOpenAI(
                model=self.model_name,
                temperature=self.temperature,
                base_url="https://router.huggingface.co/v1",
                api_key=self.hf_token,
                max_retries=3,
                timeout=120,  # —É–≤–µ–ª–∏—á–µ–Ω —Ç–∞–π–º–∞—É—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π (72B)
                default_headers={
                    "x-use-cache": "false"  # –æ—Ç–∫–ª—é—á–∏–ª–∏ –∫–µ—à –¥–ª—è —Å–≤–µ–∂–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤
                }
            )
            logger.info(f"LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.model_name} (—á–µ—Ä–µ–∑ HF Router)")
        except Exception as e:
            raise RuntimeError(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LLM {self.model_name}: {e}\n"
                "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:\n"
                "1. HF_TOKEN –∏–ª–∏ HUGGINGFACE_API_TOKEN —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –≤–∞–ª–∏–¥–µ–Ω\n"
                "2. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ —á–µ—Ä–µ–∑ Hugging Face Router API\n"
                "3. –£ –≤–∞—Å –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ –º–æ–¥–µ–ª–∏ (–º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –ø—Ä–∏–Ω—è—Ç—å —É—Å–ª–æ–≤–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)\n"
                "4. –î–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π (72B) –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –æ—Ç–≤–µ—Ç"
            ) from e
    
    def _init_chain(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-—Ü–µ–ø–æ—á–∫–∏"""
        if self.enable_memory and self.memory:
            prompt = ChatPromptTemplate.from_messages([
                ("system", SYSTEM_PROMPT),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{question}")
            ])
        else:
            # –µ—Å–ª–∏ –±–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞, —Ç–æ —É–±–∏—Ä–∞–µ–º {chat_history} –∏–∑ –ø—Ä–æ–º–ø—Ç–∞
            system_prompt_no_history = SYSTEM_PROMPT.replace("{chat_history}\n\n", "")
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt_no_history),
                ("human", "{question}")
            ])
        
        # —Ü–µ–ø–æ—á–∫–∞ retriever-format_docs-prompt-llm-parser
        if self.enable_memory and self.memory:
            # –∏—Å–ø–æ–ª—å–∑—É–µ–º RunnableLambda –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
            # —Ñ—É–Ω–∫—Ü–∏—è –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∏–∑ memory
            def get_chat_history(_):
                """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –∏–∑ memory."""
                if self.memory:
                    return self.memory.messages
                return []
            
            chat_history_lambda = RunnableLambda(get_chat_history)
            
            from operator import itemgetter
            
            self.chain = (
                {
                    "context": itemgetter("question") | self.retriever | self._format_docs,
                    "question": itemgetter("question"),
                    "chat_history": chat_history_lambda
                }
                | prompt
                | self.llm
                | StrOutputParser()
            )
        else:
            self.chain = (
                {
                    "context": self.retriever | self._format_docs,
                    "question": RunnablePassthrough()
                }
                | prompt
                | self.llm
                | StrOutputParser()
            )
    
    def _format_docs(self, docs: List[Document]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        formatted = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get("source", "unknown")
            name = doc.metadata.get("name", source)
            revision = doc.metadata.get("revision", "unknown")
            # –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º chunk_index –≤ —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ –æ–Ω —á–∏—Å–ª–æ
            chunk_idx = doc.metadata.get("chunk_index", "?")
            if isinstance(chunk_idx, int):
                chunk_idx = str(chunk_idx)
            elif chunk_idx is None:
                chunk_idx = "?"
            doc_type = doc.metadata.get("type", "unknown")
            formatted.append(
                f"[–î–æ–∫—É–º–µ–Ω—Ç {i}]\n"
                f"–ù–∞–∑–≤–∞–Ω–∏–µ: {name}\n"
                f"–¢–∏–ø: {doc_type}\n"
                f"–†–µ–≤–∏–∑–∏—è: {revision}\n"
                f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source} (—á–∞–Ω–∫ {chunk_idx})\n"
                f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{doc.page_content}\n"
            )
        
        return "\n---\n".join(formatted)
    
    def _simplify_query(self, question: str) -> str:
        """
        –£–ø—Ä–æ—â–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞,
        —É–¥–∞–ª—è–µ—Ç —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ —Ñ—Ä–∞–∑—ã, –æ—Å—Ç–∞–≤–ª—è—è –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        """
        stop_phrases = [
            "—Å–ø–∏—Å–æ–∫", "–ø–µ—Ä–µ—á–∏—Å–ª–∏", "–Ω–∞–∑–æ–≤–∏", "—É–∫–∞–∂–∏", "–ø—Ä–∏–≤–µ–¥–∏", "–¥–∞–π",
            "—Å –∏—Ö", "—Å —É–∫–∞–∑–∞–Ω–∏–µ–º", "–≤–∫–ª—é—á–∞—è", "—Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏", "–∫–∞–∫–∏–µ",
            "—á—Ç–æ —Ç–∞–∫–æ–µ", "—Ä–∞—Å—Å–∫–∞–∂–∏", "–æ–±—ä—è—Å–Ω–∏ –ø—Ä–æ"
        ]
        
        stop_words = ["–∏", "–∏–ª–∏", "–¥–ª—è", "–ø—Ä–∏", "—Å", "—Å–æ", "–∏—Ö", "–µ–≥–æ", "–µ—ë"]
        
        simplified = question.lower().strip()

        for phrase in stop_phrases:
            simplified = simplified.replace(phrase, "")
        
        words = simplified.split()

        if len(words) > 2:
            words = [w for w in words if w not in stop_words]
        
        simplified = " ".join(words)
        
        if len(simplified.split()) < 2:
            return question
        
        return simplified
    
    def ask(
        self,
        question: str,
        filters: Optional[Dict[str, Any]] = None,
        trace_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        –ó–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –∏ –ø–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ RAG
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            filters: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, {"type": "–ì–û–°–¢"})
            trace_id: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π ID —Ç—Ä–µ–π—Å–∞ –¥–ª—è Langfuse (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏:
            {
                "answer": str,
                "sources": List[Dict],
                "question": str,
                "trace_id": str (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Langfuse)
            }
        """

        search_query = self._simplify_query(question)
        logger.debug(f"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {question}, —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π: {search_query}")
        
        trace = None
        trace_id_used = None
        if self.enable_langfuse and self.langfuse:

            trace_context = None
            if trace_id:
                trace_context = {"trace_id": trace_id}
            
            trace = self.langfuse.start_span(
                name="engineer-rag",
                input={"question": question, "filters": filters},
                trace_context=trace_context
            )
            trace_id_used = trace.id if hasattr(trace, 'id') else None

        if filters:
            self.retriever = self.vectorstore.as_retriever(
                search_kwargs={"k": self.k, "filter": filters}
            )
            self._init_chain()

        retrieval_span = None
        if trace:
            retrieval_span = trace.start_span(
                name="retrieval",
                input={"original_query": question, "search_query": search_query, "k": self.k, "filters": filters}
            )
        
        retrieval_start = time.time()
        # –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        try:
            docs = self.retriever.invoke(search_query)
        except AttributeError:
            docs = self.retriever.get_relevant_documents(search_query)
        retrieval_time = time.time() - retrieval_start
        
        sources = []
        sources_metadata = []
        for doc in docs:
            chunk_idx = doc.metadata.get("chunk_index", "?")
            if isinstance(chunk_idx, int):
                chunk_idx = str(chunk_idx)
            elif chunk_idx is None:
                chunk_idx = "?"
            
            source_info = {
                "source": doc.metadata.get("source", "unknown"),
                "name": doc.metadata.get("name", doc.metadata.get("source", "unknown")),
                "revision": doc.metadata.get("revision", "unknown"),
                "type": doc.metadata.get("type", "unknown"),
                "category": doc.metadata.get("category", "unknown"),
                "chunk_index": chunk_idx,
                "snippet": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
            }
            sources.append(source_info)
            sources_metadata.append({
                "metadata": doc.metadata,
                "content_preview": doc.page_content[:200]
            })
        
        if retrieval_span:
            retrieval_span.update(
                output={
                    "documents_count": len(docs),
                    "sources": sources_metadata,
                    "retrieval_time_seconds": retrieval_time
                },
                metadata={
                    "k": self.k,
                    "filters": filters
                }
            )
            retrieval_span.end()
        
        generation_span = None
        if trace:
            context = self._format_docs(docs)
            generation_span = trace.start_span(
                name="generation",
                input={
                    "question": question,
                    "context_length": len(context),
                    "context_preview": context[:500] + "..." if len(context) > 500 else context
                }
            )
        
        generation_start = time.time()

        try:
            if self.enable_memory and self.memory:
                history_count = len(self.memory.messages) if self.memory else 0
                logger.debug(f"–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –ø–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º: {history_count} —Å–æ–æ–±—â–µ–Ω–∏–π")
                if history_count > 0:
                    logger.debug(f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {[msg.content[:50] for msg in self.memory.messages[-2:]]}")
                
                answer = self.chain.invoke({"question": question})
            else:
                answer = self.chain.invoke(question)

            if self.enable_memory and self.memory:
                self.memory.add_user_message(question)
                self.memory.add_ai_message(answer)
                logger.debug(f"–ò—Å—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞. –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ memory: {len(self.memory.messages)}")
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}"
            logger.error(error_msg)
            if generation_span:
                generation_span.update(
                    output={"error": error_msg},
                    level="ERROR"
                )
                generation_span.end()
            if trace:
                trace.update(output={"error": error_msg}, level="ERROR")
                trace.end()
            raise RuntimeError(error_msg) from e
        
        generation_time = time.time() - generation_start
        
        if generation_span:
            generation_span.update(
                output={
                    "answer": answer,
                    "answer_length": len(answer),
                    "generation_time_seconds": generation_time
                },
                metadata={
                    "model": self.model_name,
                    "temperature": self.temperature
                }
            )
            generation_span.end()

        result = {
            "answer": answer,
            "sources": sources,
            "question": question
        }
        
        if trace:
            trace.update(
                output=result,
                metadata={
                    "total_time_seconds": retrieval_time + generation_time,
                    "retrieval_time_seconds": retrieval_time,
                    "generation_time_seconds": generation_time,
                    "sources_count": len(sources),
                    "model": self.model_name
                }
            )
            trace.end()
            if not trace_id_used:
                trace_id_used = trace.id if hasattr(trace, 'id') else None
            result["trace_id"] = trace_id_used
        
        if filters:
            self.retriever = self.vectorstore.as_retriever(
                search_kwargs={"k": self.k}
            )
            self._init_chain()
        
        return result
    
    def clear_memory(self):
        """–û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞."""
        if self.memory:
            self.memory.clear()
            logger.info("–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –æ—á–∏—â–µ–Ω–∞")
    
    def get_memory_history(self) -> List[Dict[str, str]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞."""
        if not self.memory:
            return []
        
        history = []
        messages = self.memory.messages
        for i in range(0, len(messages), 2):
            if i + 1 < len(messages):
                history.append({
                    "question": messages[i].content if hasattr(messages[i], 'content') else str(messages[i]),
                    "answer": messages[i + 1].content if hasattr(messages[i + 1], 'content') else str(messages[i + 1])
                })
        return history
    
    def search_only(self, query: str, k: Optional[int] = None) -> List[Document]:
        """
        –¢–æ–ª—å–∫–æ –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è self.k)
        
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if k is not None:
            retriever = self.vectorstore.as_retriever(search_kwargs={"k": k})
            try:
                return retriever.invoke(query)
            except AttributeError:
                return retriever.get_relevant_documents(query)
        try:
            return self.retriever.invoke(query)
        except AttributeError:
            return self.retriever.get_relevant_documents(query)
    
    def add_score(
        self,
        trace_id: str,
        score_name: str = "quality",
        value: float = 1.0,
        comment: Optional[str] = None
    ):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É (score) –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É trace –≤ Langfuse.
        
        Args:
            trace_id: ID —Ç—Ä–µ–π—Å–∞
            score_name: –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "quality", "relevance")
            value: –ó–Ω–∞—á–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ (–æ–±—ã—á–Ω–æ 0.0-1.0 –∏–ª–∏ 1-5)
            comment: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
        """
        if not self.enable_langfuse or not self.langfuse:
            print("Langfuse –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, –æ—Ü–µ–Ω–∫–∞ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω–∞")
            return
        
        try:
            self.langfuse.create_score(
                trace_id=trace_id,
                name=score_name,
                value=value,
                comment=comment,
                data_type="NUMERIC"  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —á–∏—Å–ª–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
            )
            print(f"–û—Ü–µ–Ω–∫–∞ '{score_name}' = {value} –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∫ trace {trace_id}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –æ—Ü–µ–Ω–∫–∏: {e}")
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –æ—Ü–µ–Ω–∫–∏: {e}")


def create_rag_service(
    model_name: Optional[str] = None,
    temperature: float = 0.1,
    k: int = 5,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 3 –¥–æ 5 –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
    enable_langfuse: bool = True,
    hf_token: Optional[str] = None,
    enable_memory: bool = True
) -> RAGService:
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä RAGService.
    
    Args:
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Hugging Face
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        enable_langfuse: –í–∫–ª—é—á–∏—Ç—å —Ç—Ä–µ–π—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Langfuse
        hf_token: Hugging Face API —Ç–æ–∫–µ–Ω
        enable_memory: –í–∫–ª—é—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞
    
    Returns:
        –≠–∫–∑–µ–º–ø–ª—è—Ä RAGService
    """
    return RAGService(
        model_name=model_name,
        temperature=temperature,
        k=k,
        enable_langfuse=enable_langfuse,
        hf_token=hf_token,
        enable_memory=enable_memory
    )


if __name__ == "__main__":
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-—Å–µ—Ä–≤–∏—Å–∞...")
    
    model_name = os.getenv("HF_LLM_MODEL", DEFAULT_LLM_MODEL)
    
    try:
        service = create_rag_service(model_name=model_name)
        print(f"RAG-—Å–µ—Ä–≤–∏—Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–º–æ–¥–µ–ª—å: {model_name})")
        
        question = "–ö–∞–∫–æ–π –∫–ª–∞—Å—Å –±–µ—Ç–æ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–æ–≤?"
        print(f"\n–í–æ–ø—Ä–æ—Å: {question}\n")
        
        result = service.ask(question)
        
        print(f"–û—Ç–≤–µ—Ç:\n{result['answer']}\n")
        print(f"–ò—Å—Ç–æ—á–Ω–∏–∫–∏ ({len(result['sources'])}):")
        for i, source in enumerate(result['sources'], 1):
            print(f"  {i}. {source['name']} (—Ä–µ–≤–∏–∑–∏—è: {source['revision']})")
            print(f"     {source['snippet'][:100]}...")
        
        if 'trace_id' in result:
            print(f"\nüîç Langfuse Trace ID: {result['trace_id']}")
            print("   –ú–æ–∂–µ—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å –æ—Ü–µ–Ω–∫—É —á–µ—Ä–µ–∑: service.add_score(trace_id, 'quality', 1.0)")
    
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")
        print("\n–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:")
        print("  1. –í—ã–ø–æ–ª–Ω–µ–Ω ingest.py –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã")
        print("  2. HUGGINGFACE_API_TOKEN —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–ø–æ–ª—É—á–∏—Ç–µ –Ω–∞ https://huggingface.co/settings/tokens)")
        print("  3. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ —á–µ—Ä–µ–∑ Inference API")
        print("  4. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ requirements.txt")
        import traceback
        traceback.print_exc()
